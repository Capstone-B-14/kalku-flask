{"cells":[{"cell_type":"code","execution_count":49,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-28T11:18:01.041205Z","iopub.status.busy":"2023-11-28T11:18:01.040813Z","iopub.status.idle":"2023-11-28T11:18:01.050652Z","shell.execute_reply":"2023-11-28T11:18:01.049682Z","shell.execute_reply.started":"2023-11-28T11:18:01.041174Z"},"trusted":true},"outputs":[],"source":["import os, cv2\n","import numpy as np\n","import pandas as pd\n","import random, tqdm\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import albumentations as album"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:01.052606Z","iopub.status.busy":"2023-11-28T11:18:01.052331Z","iopub.status.idle":"2023-11-28T11:18:01.064053Z","shell.execute_reply":"2023-11-28T11:18:01.063198Z","shell.execute_reply.started":"2023-11-28T11:18:01.052582Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:01.065638Z","iopub.status.busy":"2023-11-28T11:18:01.065224Z","iopub.status.idle":"2023-11-28T11:18:13.130386Z","shell.execute_reply":"2023-11-28T11:18:13.129223Z","shell.execute_reply.started":"2023-11-28T11:18:01.065576Z"},"trusted":true},"outputs":[],"source":["!pip install -q -U segmentation-models-pytorch==0.2.1 albumentations > /dev/null\n","import segmentation_models_pytorch as smp"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:13.132310Z","iopub.status.busy":"2023-11-28T11:18:13.131986Z","iopub.status.idle":"2023-11-28T11:18:13.138630Z","shell.execute_reply":"2023-11-28T11:18:13.137809Z","shell.execute_reply.started":"2023-11-28T11:18:13.132279Z"},"trusted":true},"outputs":[],"source":["DATA_DIR = '../input/kalkulembu-semantic-dataset/kalkulembu-semantic-dataset/'\n","\n","x_train_dir = os.path.join(DATA_DIR, 'train')\n","y_train_dir = os.path.join(DATA_DIR, 'train_labels')\n","\n","x_valid_dir = os.path.join(DATA_DIR, 'val')\n","y_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n","\n","x_test_dir = os.path.join(DATA_DIR, 'test')\n","y_test_dir = os.path.join(DATA_DIR, 'test_labels')"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:13.141762Z","iopub.status.busy":"2023-11-28T11:18:13.141465Z","iopub.status.idle":"2023-11-28T11:18:13.162747Z","shell.execute_reply":"2023-11-28T11:18:13.161492Z","shell.execute_reply.started":"2023-11-28T11:18:13.141738Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["All dataset classes and their corresponding RGB values in labels:\n","Class Names:  ['sticker', 'cattle', 'background']\n","Class RGB values:  [[0, 117, 255], [255, 30, 249], [0, 255, 193]]\n"]}],"source":["class_dict = pd.read_csv(\"../input/kalkulembu-semantic-dataset/kalkulembu-semantic-dataset/label_class_dict.csv\")\n","# Get class names\n","class_names = class_dict['name'].tolist()\n","# Get class RGB values\n","class_rgb_values = class_dict[['r','g','b']].values.tolist()\n","\n","print('All dataset classes and their corresponding RGB values in labels:')\n","print('Class Names: ', class_names)\n","print('Class RGB values: ', class_rgb_values)"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:13.164129Z","iopub.status.busy":"2023-11-28T11:18:13.163870Z","iopub.status.idle":"2023-11-28T11:18:13.174096Z","shell.execute_reply":"2023-11-28T11:18:13.173244Z","shell.execute_reply.started":"2023-11-28T11:18:13.164106Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected classes and their corresponding RGB values in labels:\n","Class Names:  ['sticker', 'cattle', 'background']\n","Class RGB values:  [[0, 117, 255], [255, 30, 249], [0, 255, 193]]\n"]}],"source":["# Useful to shortlist specific classes in datasets with large number of classes\n","select_classes = ['sticker', 'cattle', 'background']\n","\n","# Get RGB values of required classes\n","select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n","select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n","\n","print('Selected classes and their corresponding RGB values in labels:')\n","print('Class Names: ', class_names)\n","print('Class RGB values: ', class_rgb_values)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:13.175525Z","iopub.status.busy":"2023-11-28T11:18:13.175191Z","iopub.status.idle":"2023-11-28T11:18:13.187098Z","shell.execute_reply":"2023-11-28T11:18:13.186137Z","shell.execute_reply.started":"2023-11-28T11:18:13.175492Z"},"trusted":true},"outputs":[],"source":["# helper function for data visualization\n","def visualize(**images):\n","    \"\"\"\n","    Plot images in one row\n","    \"\"\"\n","    n_images = len(images)\n","    plt.figure(figsize=(20,8))\n","    for idx, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n_images, idx + 1)\n","        plt.xticks([]); \n","        plt.yticks([])\n","        # get title from the parameter names\n","        plt.title(name.replace('_',' ').title(), fontsize=20)\n","        plt.imshow(image)\n","    plt.show()\n","\n","# Perform one hot encoding on label\n","def one_hot_encode(label, label_values):\n","    \"\"\"\n","    Convert a segmentation image label array to one-hot format\n","    by replacing each pixel value with a vector of length num_classes\n","    # Arguments\n","        label: The 2D array segmentation image label\n","        label_values\n","        \n","    # Returns\n","        A 2D array with the same width and hieght as the input, but\n","        with a depth size of num_classes\n","    \"\"\"\n","    semantic_map = []\n","    for colour in label_values:\n","        equality = np.equal(label, colour)\n","        class_map = np.all(equality, axis = -1)\n","        semantic_map.append(class_map)\n","    semantic_map = np.stack(semantic_map, axis=-1)\n","\n","    return semantic_map\n","    \n","# Perform reverse one-hot-encoding on labels / preds\n","def reverse_one_hot(image):\n","    \"\"\"\n","    Transform a 2D array in one-hot format (depth is num_classes),\n","    to a 2D array with only 1 channel, where each pixel value is\n","    the classified class key.\n","    # Arguments\n","        image: The one-hot format image \n","        \n","    # Returns\n","        A 2D array with the same width and hieght as the input, but\n","        with a depth size of 1, where each pixel value is the classified \n","        class key.\n","    \"\"\"\n","    x = np.argmax(image, axis = -1)\n","    return x\n","\n","# Perform colour coding on the reverse-one-hot outputs\n","def colour_code_segmentation(image, label_values):\n","    \"\"\"\n","    Given a 1-channel array of class keys, colour code the segmentation results.\n","    # Arguments\n","        image: single channel array where each value represents the class key.\n","        label_values\n","\n","    # Returns\n","        Colour coded image for segmentation visualization\n","    \"\"\"\n","    colour_codes = np.array(label_values)\n","    x = colour_codes[image.astype(int)]\n","\n","    return x"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:13.188389Z","iopub.status.busy":"2023-11-28T11:18:13.188147Z","iopub.status.idle":"2023-11-28T11:18:13.206656Z","shell.execute_reply":"2023-11-28T11:18:13.205917Z","shell.execute_reply.started":"2023-11-28T11:18:13.188368Z"},"trusted":true},"outputs":[],"source":["class BuildingsDataset(torch.utils.data.Dataset):\n","    \"\"\"   \n","    Args:\n","        images_dir (str): path to images folder\n","        masks_dir (str): path to segmentation masks folder\n","        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n","        augmentation (albumentations.Compose): data transfromation pipeline \n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing \n","            (e.g. noralization, shape manipulation, etc.)\n","    \"\"\"\n","    \n","    def __init__(\n","            self, \n","            images_dir, \n","            masks_dir, \n","            class_rgb_values=None, \n","            augmentation=None, \n","            preprocessing=None,\n","    ):\n","        \n","        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n","        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n","\n","        self.class_rgb_values = class_rgb_values\n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","    \n","    def __getitem__(self, i):\n","        \n","        # read images and masks\n","        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n","        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n","        \n","        # one-hot-encode the mask\n","        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n","        \n","        # apply augmentations\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","        \n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","            \n","        return image, mask\n","        \n","    def __len__(self):\n","        # return length of \n","        return len(self.image_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:13.208249Z","iopub.status.busy":"2023-11-28T11:18:13.207886Z","iopub.status.idle":"2023-11-28T11:18:14.998053Z","shell.execute_reply":"2023-11-28T11:18:14.997017Z","shell.execute_reply.started":"2023-11-28T11:18:13.208224Z"},"trusted":true},"outputs":[],"source":["dataset = BuildingsDataset(x_train_dir, y_train_dir, class_rgb_values=select_class_rgb_values)\n","random_idx = random.randint(0, len(dataset)-1)\n","image, mask = dataset[2]\n","\n","visualize(\n","    original_image = image,\n","    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n","    one_hot_encoded_mask = reverse_one_hot(mask)\n",")"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:14.999761Z","iopub.status.busy":"2023-11-28T11:18:14.999406Z","iopub.status.idle":"2023-11-28T11:18:15.008892Z","shell.execute_reply":"2023-11-28T11:18:15.007984Z","shell.execute_reply.started":"2023-11-28T11:18:14.999731Z"},"trusted":true},"outputs":[],"source":["def get_training_augmentation():\n","    train_transform = [    \n","        album.Compose(\n","            [\n","                album.HorizontalFlip(p=1),\n","                album.VerticalFlip(p=1),\n","            ],\n","            p=1,\n","        ),\n","    ]\n","    return album.Compose(train_transform)\n","\n","\n","def get_validation_augmentation():   \n","    # Add sufficient padding to ensure image is divisible by 32\n","    test_transform = [\n","        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n","    ]\n","    return album.Compose(test_transform)\n","\n","\n","def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","\n","def get_preprocessing(preprocessing_fn=None):\n","    \"\"\"Construct preprocessing transform    \n","    Args:\n","        preprocessing_fn (callable): data normalization function \n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose\n","    \"\"\"   \n","    _transform = []\n","    if preprocessing_fn:\n","        _transform.append(album.Lambda(image=preprocessing_fn))\n","    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n","        \n","    return album.Compose(_transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:15.010375Z","iopub.status.busy":"2023-11-28T11:18:15.010073Z","iopub.status.idle":"2023-11-28T11:18:20.194945Z","shell.execute_reply":"2023-11-28T11:18:20.194009Z","shell.execute_reply.started":"2023-11-28T11:18:15.010349Z"},"trusted":true},"outputs":[],"source":["augmented_dataset = BuildingsDataset(\n","    x_train_dir, y_train_dir, \n","    augmentation=get_training_augmentation(),\n","    class_rgb_values=select_class_rgb_values,\n",")\n","\n","random_idx = random.randint(0, len(augmented_dataset)-1)\n","\n","# Different augmentations on a random image/mask pair (256*256 crop)\n","for i in range(3):\n","    image, mask = augmented_dataset[random_idx]\n","    visualize(\n","        original_image = image,\n","        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n","        one_hot_encoded_mask = reverse_one_hot(mask)\n","    )"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:20.196750Z","iopub.status.busy":"2023-11-28T11:18:20.196348Z","iopub.status.idle":"2023-11-28T11:18:21.349676Z","shell.execute_reply":"2023-11-28T11:18:21.348752Z","shell.execute_reply.started":"2023-11-28T11:18:20.196712Z"},"trusted":true},"outputs":[],"source":["ENCODER = 'resnet101'\n","ENCODER_WEIGHTS = 'imagenet'\n","CLASSES = class_names\n","ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n","\n","# create segmentation model with pretrained encoder\n","model = smp.DeepLabV3Plus(\n","    encoder_name=ENCODER, \n","    encoder_weights=ENCODER_WEIGHTS, \n","    classes=len(CLASSES), \n","    activation=ACTIVATION,\n",")\n","\n","preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:21.351126Z","iopub.status.busy":"2023-11-28T11:18:21.350852Z","iopub.status.idle":"2023-11-28T11:18:21.373875Z","shell.execute_reply":"2023-11-28T11:18:21.373163Z","shell.execute_reply.started":"2023-11-28T11:18:21.351102Z"},"trusted":true},"outputs":[],"source":["# Get train and val dataset instances\n","train_dataset = BuildingsDataset(\n","    x_train_dir, y_train_dir, \n","    augmentation=get_training_augmentation(),\n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    class_rgb_values=select_class_rgb_values,\n",")\n","\n","valid_dataset = BuildingsDataset(\n","    x_valid_dir, y_valid_dir, \n","    augmentation=get_validation_augmentation(), \n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    class_rgb_values=select_class_rgb_values,\n",")\n","\n","# Get train and val data loaders\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=5)\n","valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-28T11:18:21.377492Z","iopub.status.busy":"2023-11-28T11:18:21.377213Z","iopub.status.idle":"2023-11-28T11:18:21.538578Z","shell.execute_reply":"2023-11-28T11:18:21.537230Z","shell.execute_reply.started":"2023-11-28T11:18:21.377467Z"},"trusted":true},"outputs":[],"source":["# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n","TRAINING = False\n","\n","# Set num of epochs\n","EPOCHS = 400\n","\n","# Set device: `cuda` or `cpu`\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# define loss function\n","loss = smp.utils.losses.DiceLoss()\n","\n","# define metrics\n","metrics = [\n","    smp.utils.metrics.IoU(threshold=0.5),\n","]\n","\n","# define optimizer\n","optimizer = torch.optim.Adam([ \n","    dict(params=model.parameters(), lr=0.0001),\n","])\n","\n","# define learning rate scheduler (not used in this NB)\n","lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n","    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",")\n","\n","# load best saved model checkpoint from previous commit (if present)\n","if os.path.exists('../input/kalkulembu-semantic-dataset/deeplabv3p_model_epoch_103.pth'):\n","    model = torch.load('../input/kalkulembu-semantic-dataset/deeplabv3p_model_epoch_103.pth', map_location=DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.539233Z","iopub.status.idle":"2023-11-28T11:18:21.539576Z","shell.execute_reply":"2023-11-28T11:18:21.539426Z","shell.execute_reply.started":"2023-11-28T11:18:21.539410Z"},"trusted":true},"outputs":[],"source":["train_epoch = smp.utils.train.TrainEpoch(\n","    model, \n","    loss=loss, \n","    metrics=metrics, \n","    optimizer=optimizer,\n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","valid_epoch = smp.utils.train.ValidEpoch(\n","    model, \n","    loss=loss, \n","    metrics=metrics, \n","    device=DEVICE,\n","    verbose=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.540985Z","iopub.status.idle":"2023-11-28T11:18:21.541296Z","shell.execute_reply":"2023-11-28T11:18:21.541156Z","shell.execute_reply.started":"2023-11-28T11:18:21.541141Z"},"trusted":true},"outputs":[],"source":["%%time\n","\n","if TRAINING:\n","\n","    best_iou_score = 0.0\n","    train_logs_list, valid_logs_list = [], []\n","\n","    for i in range(0, EPOCHS):\n","\n","        # Perform training & validation\n","        print('\\nEpoch: {}'.format(i))\n","        train_logs = train_epoch.run(train_loader)\n","        valid_logs = valid_epoch.run(valid_loader)\n","        train_logs_list.append(train_logs)\n","        valid_logs_list.append(valid_logs)\n","\n","        # Save model if a better val IoU score is obtained\n","        if best_iou_score < valid_logs['iou_score']:\n","            best_iou_score = valid_logs['iou_score']\n","            torch.save(model, './best_model.pth')\n","            print('Model saved!')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.542172Z","iopub.status.idle":"2023-11-28T11:18:21.542496Z","shell.execute_reply":"2023-11-28T11:18:21.542350Z","shell.execute_reply.started":"2023-11-28T11:18:21.542334Z"},"trusted":true},"outputs":[],"source":["# load best saved model checkpoint from the current run\n","if os.path.exists('../input/kalkulembu-semantic-dataset/deeplabv3p_model_epoch_103.pth'):\n","    best_model = torch.load('/kaggle/input/kalkulembu-semantic-dataset/deeplabv3p_model_epoch_103.pth', map_location=DEVICE)\n","    print('Loaded DeepLabV3+ model from this run.')\n","\n","# load best saved model checkpoint from previous commit (if present)\n","elif os.path.exists('../input/kalkulembu-semantic-dataset/deeplabv3p_model_epoch_103.pth'):\n","    best_model = torch.load('/kaggle/input/kalkulembu-semantic-dataset/deeplabv3p_model_epoch_103.pth', map_location=DEVICE)\n","    print('Loaded DeepLabV3+ model from a previous commit.')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.544342Z","iopub.status.idle":"2023-11-28T11:18:21.544807Z","shell.execute_reply":"2023-11-28T11:18:21.544581Z","shell.execute_reply.started":"2023-11-28T11:18:21.544560Z"},"trusted":true},"outputs":[],"source":["# create test dataloader (with preprocessing operation: to_tensor(...))\n","test_dataset = BuildingsDataset(\n","    x_test_dir, \n","    y_test_dir, \n","    augmentation=get_validation_augmentation(), \n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    class_rgb_values=select_class_rgb_values,\n",")\n","\n","test_dataloader = DataLoader(test_dataset)\n","\n","# test dataset for visualization (without preprocessing transformations)\n","test_dataset_vis = BuildingsDataset(\n","    x_test_dir, y_test_dir, \n","    augmentation=get_validation_augmentation(),\n","    class_rgb_values=select_class_rgb_values,\n",")\n","\n","# get a random test image/mask index\n","random_idx = random.randint(0, len(test_dataset_vis)-1)\n","image, mask = test_dataset_vis[random_idx]\n","\n","visualize(\n","    original_image = image,\n","    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n","    one_hot_encoded_mask = reverse_one_hot(mask)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.546115Z","iopub.status.idle":"2023-11-28T11:18:21.546455Z","shell.execute_reply":"2023-11-28T11:18:21.546307Z","shell.execute_reply.started":"2023-11-28T11:18:21.546291Z"},"trusted":true},"outputs":[],"source":["# Center crop padded image / mask to original image dims\n","def crop_image(image, target_image_dims=[1500,1500,3]):\n","   \n","    target_size = target_image_dims[0]\n","    image_size = len(image)\n","    padding = (image_size - target_size) // 2\n","\n","    return image[\n","        padding:image_size - padding,\n","        padding:image_size - padding,\n","        :,\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.547607Z","iopub.status.idle":"2023-11-28T11:18:21.547972Z","shell.execute_reply":"2023-11-28T11:18:21.547823Z","shell.execute_reply.started":"2023-11-28T11:18:21.547806Z"},"trusted":true},"outputs":[],"source":["sample_preds_folder = 'sample_predictions/'\n","if not os.path.exists(sample_preds_folder):\n","    os.makedirs(sample_preds_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.549408Z","iopub.status.idle":"2023-11-28T11:18:21.549785Z","shell.execute_reply":"2023-11-28T11:18:21.549600Z","shell.execute_reply.started":"2023-11-28T11:18:21.549584Z"},"trusted":true},"outputs":[],"source":["for idx in range(len(test_dataset)):\n","\n","    image, gt_mask = test_dataset[idx]\n","    # image_vis = crop_image(test_dataset_vis[idx][0].astype('uint8'))\n","    image_vis = test_dataset_vis[idx][0].astype('uint8')\n","    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n","    print(x_tensor)\n","    # Predict test image\n","    pred_mask = best_model(x_tensor)\n","    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n","    # Convert pred_mask from `CHW` format to `HWC` format\n","    pred_mask = np.transpose(pred_mask,(1,2,0))\n","    # Get prediction channel corresponding to cattle and sticker\n","    pred_cattle_heatmap = pred_mask[:,:,select_classes.index('cattle')]\n","    pred_sticker_heatmap = pred_mask[:,:,select_classes.index('sticker')]\n","    # pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n","    pred_mask = colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values)\n","    # Convert gt_mask from `CHW` format to `HWC` format\n","    gt_mask = np.transpose(gt_mask,(1,2,0))\n","    # gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values))\n","    gt_mask = colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values)\n","    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n","    \n","    visualize(\n","        original_image = image_vis,\n","        ground_truth_mask = gt_mask,\n","        predicted_mask = pred_mask,\n","        predicted_cattle_heatmap = pred_cattle_heatmap,\n","        predicted_sticker_heatmap = pred_sticker_heatmap\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.551571Z","iopub.status.idle":"2023-11-28T11:18:21.552061Z","shell.execute_reply":"2023-11-28T11:18:21.551841Z","shell.execute_reply.started":"2023-11-28T11:18:21.551819Z"},"trusted":true},"outputs":[],"source":["test_epoch = smp.utils.train.ValidEpoch(\n","    model,\n","    loss=loss, \n","    metrics=metrics, \n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","valid_logs = test_epoch.run(test_dataloader)\n","print(\"Evaluation on Test Data: \")\n","print(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\n","print(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.553208Z","iopub.status.idle":"2023-11-28T11:18:21.553702Z","shell.execute_reply":"2023-11-28T11:18:21.553461Z","shell.execute_reply.started":"2023-11-28T11:18:21.553438Z"},"trusted":true},"outputs":[],"source":["train_logs_df = pd.DataFrame(train_logs_list)\n","valid_logs_df = pd.DataFrame(valid_logs_list)\n","train_logs_df.T"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.555094Z","iopub.status.idle":"2023-11-28T11:18:21.555619Z","shell.execute_reply":"2023-11-28T11:18:21.555371Z","shell.execute_reply.started":"2023-11-28T11:18:21.555346Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20,8))\n","plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\n","plt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\n","plt.xlabel('Epochs', fontsize=20)\n","plt.ylabel('IoU Score', fontsize=20)\n","plt.title('IoU Score Plot', fontsize=20)\n","plt.legend(loc='best', fontsize=16)\n","plt.grid()\n","plt.savefig('iou_score_plot.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-28T11:18:21.556727Z","iopub.status.idle":"2023-11-28T11:18:21.557205Z","shell.execute_reply":"2023-11-28T11:18:21.556974Z","shell.execute_reply.started":"2023-11-28T11:18:21.556951Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20,8))\n","plt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\n","plt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\n","plt.xlabel('Epochs', fontsize=20)\n","plt.ylabel('Dice Loss', fontsize=20)\n","plt.title('Dice Loss Plot', fontsize=20)\n","plt.legend(loc='best', fontsize=16)\n","plt.grid()\n","plt.savefig('dice_loss_plot.png')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4058065,"sourceId":7073146,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
